{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Black-Litterman Framework for Portfolio Optimization",
   "id": "606789940ae9c573"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-06-19T21:15:49.314609Z",
     "start_time": "2024-06-19T21:15:48.150238Z"
    }
   },
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of stock tickers\n",
    "tickers = ['AAPL', 'MSFT', 'GOOG', 'ENPH', 'KO', 'AMD', 'CRM', 'VZ', 'BX', 'F']\n",
    "\n",
    "# Download historical data for the selected tickers\n",
    "data = yf.download(tickers, start=\"2021-09-30\", end=\"2022-09-30\")['Adj Close']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Convert normalized data back to a DataFrame\n",
    "data_normalized = pd.DataFrame(data_normalized, columns=tickers, index=data.index)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(data_normalized) * 0.8)\n",
    "train_data = data_normalized[:train_size]\n",
    "test_data = data_normalized[train_size:]\n",
    "\n",
    "# Plot the normalized data\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in data_normalized.columns:\n",
    "    plt.plot(data_normalized.index, data_normalized[col], label=col)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cb7633e378a8862c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:15:49.330609Z",
     "start_time": "2024-06-19T21:15:49.315619Z"
    }
   },
   "source": [],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analysis:\n",
    "Lets perform some analysis and print some graphs for better interpritation of the data."
   ],
   "id": "68446a2f17cc1a37"
  },
  {
   "cell_type": "code",
   "id": "651a740866f1033f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:15:56.574678Z",
     "start_time": "2024-06-19T21:15:49.331609Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, spearmanr, wilcoxon\n",
    "\n",
    "# Load data\n",
    "sp500 = yf.Ticker(\"^GSPC\")\n",
    "data = sp500.history(period=\"10y\")\n",
    "\n",
    "# Summary statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Data normality test (Shapiro-Wilk Test)\n",
    "stat, p = shapiro(data['Close'])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "# Spearman correlation test\n",
    "corr, _ = spearmanr(data['Open'], data['Close'])\n",
    "print('Spearman correlation: %.3f' % corr)\n",
    "\n",
    "# Visualization\n",
    "sns.pairplot(data)\n",
    "plt.show()\n",
    "\n",
    "# Rolling statistics\n",
    "data['Close'].rolling(window=30).mean().plot(title='30-Day Moving Average')     # Trend measure (moving average)\n",
    "data['Close'].rolling(window=30).std().plot(title='30-Day Rolling Standard Deviation')      # Volatility measure (standard deviation) \n",
    "plt.show()\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Implementation:\n",
    "\n",
    "Firstly we implement Transformer, Generator, and Discriminator for our model.\n",
    "\n",
    "1. Transfomer:\n",
    "    \n",
    "This class defines a neural network model using the Transformer architecture for time series or sequential data processing. The model includes a transformer and a fully connected (linear) layer.\n",
    "\n",
    "2. Generator:\n",
    "\n",
    "The Generator class defines the generator part of the GAN, responsible for creating fake data that resembles the real data. This class uses a simple feedforward neural network with three layers. The generator produces fake data, and the discriminator evaluates the data (real or fake). The training loop alternates between training the discriminator and the generator to improve their performance iteratively.\n",
    "\n",
    "3. Discriminator:\n",
    "\n",
    "The Discriminator class in a GAN (Generative Adversarial Network) is responsible for distinguishing between real and fake data. It acts as a binary classifier that assigns probabilities to input data indicating whether they are real (from the training data) or fake (generated by the generator).\n"
   ],
   "id": "528dc9424b08e00a"
  },
  {
   "cell_type": "code",
   "id": "68e63cf337998325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:16:00.912027Z",
     "start_time": "2024-06-19T21:15:56.576679Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Transformer model - for time series data\n",
    "class TransformerModel(nn.Module):   \n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=model_dim, nhead=num_heads, num_encoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(model_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GAN models - Generator and Discriminator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator model - binary classifier (real or fake) \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Parameters\n",
    "input_dim = len(tickers)\n",
    "model_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "output_dim = len(tickers)\n",
    "\n",
    "# Initialize models\n",
    "transformer = TransformerModel(input_dim, model_dim, num_heads, num_layers, output_dim)     # Transformer model for time series data\n",
    "generator = Generator(input_dim, output_dim)    # Generator model for GAN with feedforward neural network\n",
    "discriminator = Discriminator(output_dim)       # Discriminator model for GAN with feedforward neural network\n",
    "\n",
    "# Loss and optimizers \n",
    "# it takes the output of the generator and the real data and compares them to the real labels (1 for real data and 0 for fake data).\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for GAN\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in DataLoader(TensorDataset(torch.tensor(train_data.values, dtype=torch.float32)), batch_size=32, shuffle=True):\n",
    "        real_data = real_data[0]\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(batch_size, input_dim)\n",
    "        fake_data = generator(noise)\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        outputs = discriminator(real_data)\n",
    "        real_loss = criterion(outputs, real_labels)\n",
    "        outputs = discriminator(fake_data.detach())\n",
    "        fake_loss = criterion(outputs, fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        outputs = discriminator(fake_data)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] - D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:16:48.989387Z",
     "start_time": "2024-06-19T21:16:48.976386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# show architecture of the model \n",
    "print(transformer)\n",
    "print(generator)\n",
    "print(discriminator)\n"
   ],
   "id": "df26a613f249d5c0",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Black-Litterman Framework:\n",
    "\n",
    "The Black-Litterman model is an asset allocation model that combines the expected returns from the equilibrium market with the views of an investor to generate an optimal portfolio. The model uses the market equilibrium returns, investor views, and uncertainty in views to calculate the optimal portfolio weights. The Black-Litterman model is widely used in portfolio optimization to incorporate investor views and market equilibrium returns into the asset allocation process.\n",
    "\n",
    "The Black-Litterman model consists of the following steps:\n",
    "\n",
    "1. Calculate the implied equilibrium returns based on the market weights and covariance matrix.\n",
    "2. Generate investor views on the expected returns of assets.\n",
    "3. Calculate the uncertainty in views (Omega matrix) based on the views.\n",
    "4. Adjust the covariance matrix based on the uncertainty in views.\n",
    "5. Calculate the Black-Litterman expected returns based on the adjusted covariance matrix and investor views.\n",
    "6. Optimize the portfolio weights to maximize the Sharpe ratio.\n",
    "7. Compare the optimized portfolio with an equally weighted portfolio.\n",
    "\n",
    "The Black-Litterman expected returns formula is:\n",
    "\n",
    "![Black-Litterman Expected Returns](https://media.fe.training/2024/02/hgpjwmpr-Exepected-Returns-Image-2.png)\n",
    "\n",
    "![Black-Litterman Expected Returns Formula](https://media.fe.training/2024/02/pqm0bt20-Expected-return-formula-Image-4.png)\n"
   ],
   "id": "1985d8d148327860"
  },
  {
   "cell_type": "code",
   "id": "d1dc95ba01054710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:16:00.959025Z",
     "start_time": "2024-06-19T21:16:00.930027Z"
    }
   },
   "source": [
    "# Black-Litterman parameters\n",
    "'''\n",
    "A δ value of 2.5 indicates that the investor is willing to take on a risk level corresponding to a standard deviation of approximately 15.5% in the market returns. This means the investor is moderately risk-averse but still willing to accept a fair amount of market risk for the potential of higher returns.'''\n",
    "delta = 2.5 # risk aversion rate -> low means high risk appetite and vice-versa || equation delta =( expected return of PO - risk free rate) \n",
    "tau = 0.05 # uncertainty in views (tau = 1 / confidence level) -> low means more confidence in views || equation tau = 1 / confidence level\n",
    "risk_free_rate = 0.02 # risk-free rate of return\n",
    "\n",
    "# Calculate the implied equilibrium returns (Π)\n",
    "cov_matrix = train_data.cov()\n",
    "market_weights = np.ones(len(tickers)) / len(tickers)   # Assuming equal weights for all assets\n",
    "pi = delta * np.dot(cov_matrix, market_weights)  # Expected returns based on market weights (Implied equilibrium returns)\n",
    "\n",
    "# Generate views using the Transformer-GAN model\n",
    "views = generator(torch.randn(len(tickers), input_dim)).detach().numpy()    # Generate views using the generator model (fake data) \n",
    "\n",
    "# Calculate the Omega matrix (uncertainty in views)\n",
    "P = np.eye(len(tickers))    # Identity matrix (views are on the returns of individual assets)\n",
    "Q = views.mean(axis=0)    # Average of the generated views\n",
    "omega = np.diag(np.diag(np.dot(np.dot(P, tau * cov_matrix), P.T)))  # Uncertainty matrix (Omega) based on views\n",
    "\n",
    "# Adjusted covariance matrix (Σ̂)\n",
    "adjusted_cov_matrix = cov_matrix + np.linalg.inv(np.linalg.inv(tau * cov_matrix) + np.dot(np.dot(P.T, np.linalg.inv(omega)), P))\n",
    "\n",
    "# Black-Litterman expected returns (µBL)\n",
    "mu_bl = np.dot(np.linalg.inv(np.linalg.inv(tau * cov_matrix) + np.dot(np.dot(P.T, np.linalg.inv(omega)), P)), \n",
    "               np.dot(np.linalg.inv(tau * cov_matrix), pi) + np.dot(np.dot(P.T, np.linalg.inv(omega)), Q))\n",
    "\n",
    "# Portfolio optimization (maximizing Sharpe Ratio)\n",
    "def objective(weights):\n",
    "    return - (np.dot(weights, mu_bl) - risk_free_rate) / np.sqrt(np.dot(np.dot(weights, adjusted_cov_matrix), weights.T))\n",
    "\n",
    "constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "bounds = [(0, 1) for _ in range(len(tickers))]\n",
    "initial_guess = market_weights\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "result = minimize(objective, initial_guess, bounds=bounds, constraints=constraints)\n",
    "optimal_weights = result.x\n",
    "\n",
    "print('Optimal Portfolio Weights:', optimal_weights)\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4769982424b13652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:16:01.146062Z",
     "start_time": "2024-06-19T21:16:00.961026Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the optimal weights\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tickers, optimal_weights)\n",
    "plt.xlabel('Assets')\n",
    "plt.ylabel('Optimal Weights')\n",
    "plt.title('Optimal Portfolio Allocation')\n",
    "plt.show()\n"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "610d3e8b63d6cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:16:01.162058Z",
     "start_time": "2024-06-19T21:16:01.148068Z"
    }
   },
   "source": [
    "for i in optimal_weights:\n",
    "    # print 4 decimal places\n",
    "    print(f'{i:.4f}')\n",
    "print(optimal_weights.sum())"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3dfe8c0335ffc854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:16:01.178068Z",
     "start_time": "2024-06-19T21:16:01.164059Z"
    }
   },
   "source": [
    "# Calculate expected return and volatility of the optimized portfolio\n",
    "expected_return = np.dot(optimal_weights, mu_bl)\n",
    "portfolio_volatility = np.sqrt(np.dot(np.dot(optimal_weights, adjusted_cov_matrix), optimal_weights.T))\n",
    "sharpe_ratio = (expected_return - risk_free_rate) / portfolio_volatility\n",
    "\n",
    "print(f'Expected Return: {expected_return:.4f}')\n",
    "print(f'Portfolio Volatility: {portfolio_volatility:.4f}')\n",
    "print(f'Sharpe Ratio: {sharpe_ratio:.4f}')\n"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "382674f2b3f2566f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:16:01.477608Z",
     "start_time": "2024-06-19T21:16:01.181060Z"
    }
   },
   "source": [
    "# Calculate the performance of an equally weighted portfolio\n",
    "equal_weights = np.ones(len(tickers)) / len(tickers)\n",
    "equal_expected_return = np.dot(equal_weights, mu_bl)\n",
    "equal_portfolio_volatility = np.sqrt(np.dot(np.dot(equal_weights, adjusted_cov_matrix), equal_weights.T))\n",
    "equal_sharpe_ratio = (equal_expected_return - risk_free_rate) / equal_portfolio_volatility\n",
    "\n",
    "print('\\nBenchmark (Equally Weighted Portfolio):')\n",
    "print(f'Expected Return: {equal_expected_return:.4f}')\n",
    "print(f'Portfolio Volatility: {equal_portfolio_volatility:.4f}')\n",
    "print(f'Sharpe Ratio: {equal_sharpe_ratio:.4f}')\n",
    "\n",
    "# Comparison bar chart\n",
    "labels = ['Optimized Portfolio', 'Equally Weighted Portfolio']\n",
    "returns = [expected_return, equal_expected_return]\n",
    "volatilities = [portfolio_volatility, equal_portfolio_volatility]\n",
    "sharpe_ratios = [sharpe_ratio, equal_sharpe_ratio]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "# Expected return comparison\n",
    "ax[0].bar(x, returns, color=['blue', 'green'])\n",
    "ax[0].set_ylabel('Expected Return')\n",
    "ax[0].set_title('Expected Return Comparison')\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(labels)\n",
    "\n",
    "# Volatility comparison\n",
    "ax[1].bar(x, volatilities, color=['blue', 'green'])\n",
    "ax[1].set_ylabel('Volatility')\n",
    "ax[1].set_title('Volatility Comparison')\n",
    "ax[1].set_xticks(x)\n",
    "ax[1].set_xticklabels(labels)\n",
    "\n",
    "# Sharpe ratio comparison\n",
    "ax[2].bar(x, sharpe_ratios, color=['blue', 'green'])\n",
    "ax[2].set_ylabel('Sharpe Ratio')\n",
    "ax[2].set_title('Sharpe Ratio Comparison')\n",
    "ax[2].set_xticks(x)\n",
    "ax[2].set_xticklabels(labels)\n",
    "\n",
    "plt.show()\n"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3667a01549fc1a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T21:16:01.492621Z",
     "start_time": "2024-06-19T21:16:01.479610Z"
    }
   },
   "source": [],
   "execution_count": 10,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
