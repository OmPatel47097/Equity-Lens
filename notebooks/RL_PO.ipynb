{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Fetch Data Using Yahoo Finance\n",
    "We will fetch historical stock prices for the assets in our portfolio."
   ],
   "id": "bb69e7a5185bf811"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-13T02:59:23.779707Z",
     "start_time": "2024-06-13T02:59:22.454154Z"
    }
   },
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# List of tickers for the assets in the portfolio\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "\n",
    "# Fetch historical data for the specified tickers\n",
    "data = yf.download(tickers, start='2020-01-01', end='2023-01-01')['Adj Close']\n",
    "\n",
    "# Fill missing data\n",
    "data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Save the data to a CSV file for later use\n",
    "data.to_csv('data.csv')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  5 of 5 completed\n",
      "C:\\Users\\Om C. Patel\\AppData\\Local\\Temp\\ipykernel_29284\\2397440343.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = data.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. Define the Custom Environment:\n",
    "\n",
    "The custom environment will remain largely the same, but it will now use the Yahoo Finance data we fetched."
   ],
   "id": "3c74650327b5b142"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T03:11:28.558838Z",
     "start_time": "2024-06-13T03:11:28.482845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, data, initial_cash=10000, transaction_cost=0.0025):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.initial_cash = initial_cash\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.current_step = 0\n",
    "        self.num_assets = data.shape[1]\n",
    "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(self.num_assets + 1,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(self.num_assets + 1,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cash = self.initial_cash\n",
    "        self.asset_holdings = np.zeros(self.num_assets)\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        action = action / np.sum(action)  # Normalize the action to sum to 1\n",
    "        asset_prices = self.data.iloc[self.current_step].values\n",
    "        portfolio_value = self.cash + np.sum(self.asset_holdings * asset_prices)\n",
    "        self.cash = portfolio_value * action[0]\n",
    "        self.asset_holdings = (portfolio_value * action[1:]) / asset_prices\n",
    "        self.current_step += 1\n",
    "        reward = self._get_reward()\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        asset_prices = self.data.iloc[self.current_step].values\n",
    "        return np.concatenate(([self.cash], asset_prices))\n",
    "\n",
    "    def _get_reward(self):\n",
    "        asset_prices = self.data.iloc[self.current_step].values\n",
    "        portfolio_value = self.cash + np.sum(self.asset_holdings * asset_prices)\n",
    "        return portfolio_value - self.initial_cash\n",
    "\n",
    "# Prepare the data\n",
    "data = pd.read_csv('data.csv', index_col=0, parse_dates=True)\n",
    "env = DummyVecEnv([lambda: PortfolioEnv(data)])\n",
    "\n",
    "# Define the policy network\n",
    "class CustomPolicy(nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(CustomPolicy, self).__init__()\n",
    "        self.fc1 = nn.Linear(observation_space.shape[0], 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_space.shape[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Train the RL agent\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomPolicy,\n",
    "    features_extractor_kwargs=dict(observation_space=env.observation_space, action_space=env.action_space)\n",
    ")\n",
    "\n",
    "model = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/', log_path='./logs/', eval_freq=500, deterministic=True, render=False)\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_portfolio\")\n",
    "\n",
    "# Test the trained agent\n",
    "obs = env.reset()\n",
    "for _ in range(len(data) - 1):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n"
   ],
   "id": "c7edd5bb9024553",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python_Interpriters\\Power_Champian\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CustomPolicy.__init__() got multiple values for argument 'observation_space'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 73\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# Train the RL agent\u001B[39;00m\n\u001B[0;32m     68\u001B[0m policy_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\n\u001B[0;32m     69\u001B[0m     features_extractor_class\u001B[38;5;241m=\u001B[39mCustomPolicy,\n\u001B[0;32m     70\u001B[0m     features_extractor_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(observation_space\u001B[38;5;241m=\u001B[39menv\u001B[38;5;241m.\u001B[39mobservation_space, action_space\u001B[38;5;241m=\u001B[39menv\u001B[38;5;241m.\u001B[39maction_space)\n\u001B[0;32m     71\u001B[0m )\n\u001B[1;32m---> 73\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mPPO\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMlpPolicy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     74\u001B[0m eval_callback \u001B[38;5;241m=\u001B[39m EvalCallback(env, best_model_save_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./logs/\u001B[39m\u001B[38;5;124m'\u001B[39m, log_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./logs/\u001B[39m\u001B[38;5;124m'\u001B[39m, eval_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, deterministic\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, render\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     75\u001B[0m model\u001B[38;5;241m.\u001B[39mlearn(total_timesteps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m, callback\u001B[38;5;241m=\u001B[39meval_callback)\n",
      "File \u001B[1;32mC:\\Python_Interpriters\\Power_Champian\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:171\u001B[0m, in \u001B[0;36mPPO.__init__\u001B[1;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_kl \u001B[38;5;241m=\u001B[39m target_kl\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _init_setup_model:\n\u001B[1;32m--> 171\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python_Interpriters\\Power_Champian\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:174\u001B[0m, in \u001B[0;36mPPO._setup_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_setup_model\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 174\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    176\u001B[0m     \u001B[38;5;66;03m# Initialize schedules for policy/value clipping\u001B[39;00m\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_range \u001B[38;5;241m=\u001B[39m get_schedule_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_range)\n",
      "File \u001B[1;32mC:\\Python_Interpriters\\Power_Champian\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:134\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm._setup_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    122\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout_buffer_class \u001B[38;5;241m=\u001B[39m RolloutBuffer\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout_buffer_class(\n\u001B[0;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_steps,\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout_buffer_kwargs,\n\u001B[0;32m    133\u001B[0m )\n\u001B[1;32m--> 134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_class(  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_schedule, use_sde\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_sde, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_kwargs\n\u001B[0;32m    136\u001B[0m )\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[1;32mC:\\Python_Interpriters\\Power_Champian\\lib\\site-packages\\stable_baselines3\\common\\policies.py:507\u001B[0m, in \u001B[0;36mActorCriticPolicy.__init__\u001B[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001B[0m\n\u001B[0;32m    504\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mortho_init \u001B[38;5;241m=\u001B[39m ortho_init\n\u001B[0;32m    506\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor \u001B[38;5;241m=\u001B[39m share_features_extractor\n\u001B[1;32m--> 507\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_extractor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_features_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_extractor\u001B[38;5;241m.\u001B[39mfeatures_dim\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n",
      "File \u001B[1;32mC:\\Python_Interpriters\\Power_Champian\\lib\\site-packages\\stable_baselines3\\common\\policies.py:120\u001B[0m, in \u001B[0;36mBaseModel.make_features_extractor\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_features_extractor\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseFeaturesExtractor:\n\u001B[0;32m    119\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Helper method to create a features extractor.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_extractor_class(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_extractor_kwargs)\n",
      "\u001B[1;31mTypeError\u001B[0m: CustomPolicy.__init__() got multiple values for argument 'observation_space'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e218aa1e52914165"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
